{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import re\n",
    "from zipfile import BadZipFile\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Country List and Timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of sub-Saharan African countries (ISO Alpha-2 country codes)\n",
    "country_list = pd.read_excel('Country List.xlsx')\n",
    "blacklist_countries = country_list[country_list['IS_Africa'] == 0]['Alpha-2 code'].tolist()\n",
    "sub_saharan_countries = country_list[country_list['IS_Africa'] == 1]['Alpha-2 code'].tolist()\n",
    "\n",
    "# Date range\n",
    "start_date = datetime(2024, 1, 1)\n",
    "end_date = datetime(2024, 6, 30)\n",
    "\n",
    "# Generate a list of dates for the time range\n",
    "date_list = [(start_date + timedelta(days=x)).strftime('%Y%m%d') for x in range((end_date - start_date).days + 1)]\n",
    "time_list = ['000000', '040000', '080000', '120000', '160000', '200000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download and process GDELT GKG data for a specific date\n",
    "def process_gdelt_gkg(date, time):\n",
    "    url = f\"http://data.gdeltproject.org/gdeltv2/{date}{time}.gkg.csv.zip\"\n",
    "    zip_path = f\"{date}{time}.zip\"\n",
    "    \n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(zip_path):\n",
    "        response = requests.get(url)\n",
    "        with open(zip_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    \n",
    "    # Read the zip file\n",
    "    try:\n",
    "        # Read the zip file\n",
    "        df = pd.read_csv(zip_path, compression='zip', header=None, delimiter='\\t', encoding='latin-1')\n",
    "        return df\n",
    "    except BadZipFile:\n",
    "        print(f\"BadZipFile error encountered for {zip_path}. Skipping this file.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if a BadZipFile error occurs\n",
    "\n",
    "# Initialize an empty DataFrame to hold all data\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "# Loop through each date and process the data\n",
    "for date in date_list:\n",
    "    for time in time_list:\n",
    "        daily_data = process_gdelt_gkg(date, time)\n",
    "        all_data = pd.concat([all_data, daily_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA\n",
    "all_data.dropna(subset=[9], how='all', inplace=True)\n",
    "\n",
    "# Filter data for sub-Saharan African countries\n",
    "filtered_data = all_data[~all_data[9].str.contains('|'.join(blacklist_countries))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Title LDA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page_title(text):\n",
    "    match = re.search(r'<PAGE_TITLE>(.*?)</PAGE_TITLE>', text)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "filtered_data['Page_Title'] = filtered_data[26].apply(extract_page_title)\n",
    "filtered_data = filtered_data.fillna('')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_data['Processed_Title'] = filtered_data['Page_Title'].apply(\n",
    "    lambda x: ' '.join([word for word in word_tokenize(x.lower()) if word.isalpha() and word not in stop_words])\n",
    ")\n",
    "\n",
    "texts = [text.split() for text in filtered_data['Processed_Title']]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Perform LDA analysis\n",
    "lda_model = LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic: {idx}\\nWords: {topic}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the LDA model\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary, mds='mmds')\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each document (title) to the dominant topic\n",
    "dominant_topics = [max(lda_model[doc], key=lambda x: x[1])[0] for doc in corpus]\n",
    "\n",
    "# Add dominant topic to the original dataframe\n",
    "filtered_data['Dominant_Topic'] = dominant_topics\n",
    "\n",
    "# Function to print sample titles for each topic\n",
    "def print_sample_titles_per_topic(df, topic_num, sample_size=5):\n",
    "    print(f\"\\nSample titles for Topic {topic_num+1}:\")\n",
    "    sample_titles = df[df['Dominant_Topic'] == topic_num]['Page_Title'].sample(n=sample_size, random_state=1).tolist()\n",
    "    for title in sample_titles:\n",
    "        print(f\"- {title}\")\n",
    "\n",
    "# Print sample titles for each topic\n",
    "for topic in range(lda_model.num_topics):\n",
    "    print_sample_titles_per_topic(filtered_data, topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GKG Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the themes into separate columns\n",
    "# Create a new DataFrame to hold the expanded themes\n",
    "expanded_data = pd.DataFrame(filtered_data[7].str.split(';').tolist(), index=filtered_data.index)\n",
    "\n",
    "# Count the themes for each incident\n",
    "theme_counts = expanded_data.stack().value_counts()\n",
    "\n",
    "# Convert to DataFrame\n",
    "theme_counts_df = pd.DataFrame(theme_counts).reset_index()\n",
    "theme_counts_df.columns = ['Theme', 'Frequency']\n",
    "\n",
    "# Drop first empty row\n",
    "theme_counts_df = theme_counts_df.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top 10 themes\n",
    "top_themes = theme_counts_df.head(10)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_themes['Theme'], top_themes['Frequency'])\n",
    "plt.xlabel('Theme')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10 Themes in Sub-Saharan Africa (Jan 2024 - Jun 2024)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
